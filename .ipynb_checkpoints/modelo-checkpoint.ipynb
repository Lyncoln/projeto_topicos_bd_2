{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1119d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, sum, max, col, concat, lit, monotonically_increasing_id\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import Row\n",
    "\n",
    "\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "from fbprophet import Prophet\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6b842505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfZipWithIndex (df, offset=1, colName=\"rowId\"):\n",
    "    '''\n",
    "        Enumerates dataframe rows is native order, like rdd.ZipWithIndex(), but on a dataframe \n",
    "        and preserves a schema\n",
    "\n",
    "        :param df: source dataframe\n",
    "        :param offset: adjustment to zipWithIndex()'s index\n",
    "        :param colName: name of the index column\n",
    "    '''\n",
    "\n",
    "    new_schema = StructType(\n",
    "                    [StructField(colName,LongType(),True)]        # new added field in front\n",
    "                    + df.schema.fields                            # previous schema\n",
    "                )\n",
    "\n",
    "    zipped_rdd = df.rdd.zipWithIndex()\n",
    "\n",
    "    new_rdd = zipped_rdd.map(lambda args: ([args[1] + offset] + list(args[0])))\n",
    "\n",
    "    return spark.createDataFrame(new_rdd, new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1649958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "        StructField(\"ds\", DateType(), True),\n",
    "        StructField(\"yhat\", DoubleType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3bf80db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def fit_pandas_udf(df):\n",
    "    \"\"\"\n",
    "    :param df: Dataframe (train + test data)\n",
    "    :return: predictions as defined in the output schema\n",
    "    \"\"\"\n",
    "\n",
    "    def train_fitted_prophet(df, cutoff):\n",
    "        \n",
    "        names = df.columns\n",
    "        \n",
    "        #train\n",
    "        ts_train = (df\n",
    "                    .query('id <= @cutoff')\n",
    "                    .rename(columns={names[1]: 'ds', names[2]: 'y'})\n",
    "                    .sort_values('ds')\n",
    "                    )[['ds','y']]\n",
    "        \n",
    "        print(ts_train.columns)\n",
    "        \n",
    "        \n",
    "        # test\n",
    "        ts_test = (df\n",
    "                   .query('id > @cutoff')\n",
    "                   .rename(columns={names[1]: 'ds', names[2]: 'y'})\n",
    "                   .sort_values('ds')\n",
    "                   .assign(ds=lambda x: pd.to_datetime(x[\"ds\"]))\n",
    "                   .drop('y', axis=1)\n",
    "                   )[['ds']]\n",
    "        \n",
    "        print(ts_test.columns)\n",
    "\n",
    " \n",
    "\n",
    "        # init model\n",
    "        m = Prophet(yearly_seasonality=True,\n",
    "                    weekly_seasonality=True,\n",
    "                    daily_seasonality=True)\n",
    "        m.fit(ts_train)\n",
    "        \n",
    "        \n",
    "\n",
    "        # to date\n",
    "        \n",
    "        # at this step we predict the future and we get plenty of additional columns be cautious\n",
    "        ts_hat = (m.predict(ts_test)[[\"ds\", \"yhat\"]]\n",
    "                  .assign(ds=lambda x: pd.to_datetime(x[\"ds\"]))\n",
    "                  ).merge(ts_test, on=[\"ds\"], how=\"left\")  \n",
    "        \n",
    "\n",
    "        return pd.DataFrame(ts_hat, columns=schema.fieldNames())\n",
    "\n",
    "    return train_fitted_prophet(df, cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "37f02f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 287:=============================================>       (108 + 9) / 127]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    spark = (SparkSession\n",
    "             .builder\n",
    "             .appName(\"forecasting\")\n",
    "             .getOrCreate()\n",
    "             #.config('spark.sql.execution.arrow.enable', 'true')\n",
    "             )\n",
    "    \n",
    "    data = (spark\n",
    "                .read\n",
    "                .format(\"csv\")\n",
    "                .option('header', 'true')\n",
    "                .option('inferSchema','true')\n",
    "                .load('data_simulation.csv')\n",
    "                #.load('Downloads/AEP_hourly.csv')\n",
    "            )\n",
    "    \n",
    "    data.createOrReplaceTempView(\"data\")\n",
    "    data = spark.sql(f\"SELECT LEFT(Datetime,10) AS Datetime, {data.columns[1]}  FROM data\")\n",
    "    data = data.groupBy(\"Datetime\")\\\n",
    "               .mean(\"MW\")\\\n",
    "               .sort(col('DateTime'))\n",
    "    \n",
    "    \n",
    "    # 70% of the real dataset\n",
    "    data_length = data.count()\n",
    "    train_size = int(round(0.7 * data_length,0))\n",
    "    \n",
    "    \n",
    "    ##Add future days to predict\n",
    "    \n",
    "    #last_day = data.tail(1)[0].__getitem__(\"Datetime\")  # Não sei se é viável\n",
    "    last_day = data.tail(1)[0].asDict()['Datetime']\n",
    "    future_days = pd.date_range(start = last_day,\n",
    "                                periods = 29)\n",
    "    sequence_days = list(future_days.strftime(\"%Y-%m-%d\"))[1:-1]\n",
    "    future = spark.createDataFrame(sequence_days, \n",
    "                                   StringType())\n",
    "    future.createOrReplaceTempView(\"future\")\n",
    "    future = spark.sql(\"SELECT value AS Datetime FROM future\")\n",
    "    future = future.withColumn(data.columns[1],\n",
    "                               lit(None))\n",
    "    \n",
    "\n",
    "    \n",
    "    df = (data.union(future)).sort(col('Datetime'))\n",
    "    df = dfZipWithIndex(df,colName=\"id\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    cutoff = train_size\n",
    "    # Apply forcasting\n",
    "    global_predictions = (df\n",
    "                          .groupBy()\n",
    "                          .apply(fit_pandas_udf)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8d099d7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Index(['ds', 'y'], dtype='object')                                  (0 + 1) / 1]\n",
      "Index(['ds'], dtype='object')\n",
      "\n",
      "Initial log joint probability = -123.798\n",
      "Iteration  1. Log joint probability =    79.4266. Improved by 203.225.\n",
      "Iteration  2. Log joint probability =    121.029. Improved by 41.6029.\n",
      "Iteration  3. Log joint probability =    181.698. Improved by 60.6681.\n",
      "Iteration  4. Log joint probability =    223.977. Improved by 42.2797.\n",
      "Iteration  5. Log joint probability =    234.288. Improved by 10.3106.\n",
      "Iteration  6. Log joint probability =    237.122. Improved by 2.83383.\n",
      "Iteration  7. Log joint probability =    238.937. Improved by 1.81497.\n",
      "Iteration  8. Log joint probability =    239.109. Improved by 0.171925.\n",
      "Iteration  9. Log joint probability =    239.451. Improved by 0.342607.\n",
      "Iteration 10. Log joint probability =    239.646. Improved by 0.194713.\n",
      "Iteration 11. Log joint probability =     239.75. Improved by 0.10374.\n",
      "Iteration 12. Log joint probability =    239.915. Improved by 0.165363.\n",
      "Iteration 13. Log joint probability =    240.151. Improved by 0.235517.\n",
      "Iteration 14. Log joint probability =    240.303. Improved by 0.152211.\n",
      "Iteration 15. Log joint probability =     240.38. Improved by 0.0773459.\n",
      "Iteration 16. Log joint probability =    240.482. Improved by 0.101981.\n",
      "Iteration 17. Log joint probability =    240.554. Improved by 0.0714701.\n",
      "Iteration 18. Log joint probability =    240.558. Improved by 0.00467746.\n",
      "Iteration 19. Log joint probability =    240.565. Improved by 0.00703251.\n",
      "Iteration 20. Log joint probability =    240.607. Improved by 0.0420494.\n",
      "Iteration 21. Log joint probability =    240.638. Improved by 0.0311163.\n",
      "Iteration 22. Log joint probability =    240.649. Improved by 0.0108444.\n",
      "Iteration 23. Log joint probability =     240.68. Improved by 0.0304736.\n",
      "Iteration 24. Log joint probability =    240.702. Improved by 0.0223617.\n",
      "Iteration 25. Log joint probability =    240.853. Improved by 0.150538.\n",
      "Iteration 26. Log joint probability =     240.88. Improved by 0.0271772.\n",
      "Iteration 27. Log joint probability =    240.885. Improved by 0.00480199.\n",
      "Iteration 28. Log joint probability =    240.899. Improved by 0.0143116.\n",
      "Iteration 29. Log joint probability =    240.926. Improved by 0.0267395.\n",
      "Iteration 30. Log joint probability =    240.935. Improved by 0.00883897.\n",
      "Iteration 31. Log joint probability =     240.94. Improved by 0.00497673.\n",
      "Iteration 32. Log joint probability =     240.94. Improved by 0.00081202.\n",
      "Iteration 33. Log joint probability =    240.944. Improved by 0.00326166.\n",
      "Iteration 34. Log joint probability =    240.945. Improved by 0.00108074.\n",
      "Iteration 35. Log joint probability =    240.945. Improved by 0.0002596.\n",
      "Iteration 36. Log joint probability =    240.946. Improved by 0.00062531.\n",
      "Iteration 37. Log joint probability =    240.946. Improved by 0.000258975.\n",
      "Iteration 38. Log joint probability =    240.946. Improved by 0.000330205.\n",
      "Iteration 39. Log joint probability =    240.946. Improved by 0.000149368.\n",
      "Iteration 40. Log joint probability =    240.946. Improved by 5.73966e-05.\n",
      "Iteration 41. Log joint probability =    240.946. Improved by 5.89932e-05.\n",
      "Iteration 42. Log joint probability =    240.946. Improved by 1.75633e-05.\n",
      "Iteration 43. Log joint probability =    240.946. Improved by 2.19348e-05.\n",
      "Iteration 44. Log joint probability =    240.946. Improved by 1.40466e-05.\n",
      "Iteration 45. Log joint probability =    240.946. Improved by 3.65628e-06.\n",
      "Iteration 46. Log joint probability =    240.946. Improved by 8.6621e-06.\n",
      "Iteration 47. Log joint probability =    240.946. Improved by 4.73994e-06.\n",
      "Iteration 48. Log joint probability =    240.946. Improved by 4.8299e-06.\n",
      "Iteration 49. Log joint probability =    240.946. Improved by 2.54968e-06.\n",
      "Iteration 50. Log joint probability =    240.946. Improved by 4.57437e-07.\n",
      "Iteration 51. Log joint probability =    240.946. Improved by 1.44137e-06.\n",
      "Iteration 52. Log joint probability =    240.946. Improved by 3.04472e-08.\n",
      "Iteration 53. Log joint probability =    240.946. Improved by 6.69076e-07.\n",
      "Iteration 54. Log joint probability =    240.946. Improved by 2.22812e-07.\n",
      "Iteration 55. Log joint probability =    240.946. Improved by 2.84325e-07.\n",
      "Iteration 56. Log joint probability =    240.946. Improved by 2.00247e-07.\n",
      "Iteration 57. Log joint probability =    240.946. Improved by 5.12177e-08.\n",
      "Iteration 58. Log joint probability =    240.946. Improved by 3.79953e-08.\n",
      "Iteration 59. Log joint probability =    240.946. Improved by 4.40924e-08.\n",
      "Iteration 60. Log joint probability =    240.946. Improved by 1.63601e-08.\n",
      "Iteration 61. Log joint probability =    240.946. Improved by 9.01713e-09.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|        ds|              yhat|\n",
      "+----------+------------------+\n",
      "|2004-12-10|12906.682380411503|\n",
      "|2004-12-11|11477.671421249173|\n",
      "|2004-12-12| 10609.99888017381|\n",
      "|2004-12-13|11979.518964026262|\n",
      "|2004-12-14| 11972.48772068324|\n",
      "|2004-12-15| 12050.61727188952|\n",
      "|2004-12-16|12395.886262198313|\n",
      "|2004-12-17|13307.344100721153|\n",
      "|2004-12-18|13714.029261295087|\n",
      "|2004-12-19|15374.204216504259|\n",
      "|2004-12-20|  20050.6489498212|\n",
      "|2004-12-21| 24206.56112400464|\n",
      "|2004-12-22|29366.773784125533|\n",
      "|2004-12-23| 35758.75923743601|\n",
      "|2004-12-24| 43705.78250977809|\n",
      "|2004-12-25|52136.454855189855|\n",
      "|2004-12-26| 62781.02057443888|\n",
      "|2004-12-27| 77344.24609119965|\n",
      "|2004-12-28| 92199.57843566514|\n",
      "|2004-12-29| 108749.3909311865|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "global_predictions.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
