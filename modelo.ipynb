{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1119d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, sum, max, col, concat, lit, monotonically_increasing_id\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import Row\n",
    "\n",
    "\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "from fbprophet import Prophet\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b842505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfZipWithIndex (df, offset=1, colName=\"rowId\"):\n",
    "    '''\n",
    "        Enumerates dataframe rows is native order, like rdd.ZipWithIndex(), but on a dataframe \n",
    "        and preserves a schema\n",
    "\n",
    "        :param df: source dataframe\n",
    "        :param offset: adjustment to zipWithIndex()'s index\n",
    "        :param colName: name of the index column\n",
    "    '''\n",
    "\n",
    "    new_schema = StructType(\n",
    "                    [StructField(colName,LongType(),True)]        # new added field in front\n",
    "                    + df.schema.fields                            # previous schema\n",
    "                )\n",
    "\n",
    "    zipped_rdd = df.rdd.zipWithIndex()\n",
    "\n",
    "    new_rdd = zipped_rdd.map(lambda args: ([args[1] + offset] + list(args[0])))\n",
    "\n",
    "    return spark.createDataFrame(new_rdd, new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1649958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "        StructField(\"ds\", DateType(), True),\n",
    "        StructField(\"yhat\", DoubleType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bf80db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def fit_pandas_udf(df):\n",
    "    \"\"\"\n",
    "    :param df: Dataframe (train + test data)\n",
    "    :return: predictions as defined in the output schema\n",
    "    \"\"\"\n",
    "\n",
    "    def train_fitted_prophet(df, cutoff):\n",
    "        \n",
    "        names = df.columns\n",
    "        \n",
    "        #train\n",
    "        ts_train = (df\n",
    "                    .query('id <= @cutoff')\n",
    "                    .rename(columns={names[1]: 'ds', names[2]: 'y'})\n",
    "                    .sort_values('ds')\n",
    "                    )[['ds','y']]\n",
    "        \n",
    "        print(ts_train.columns)\n",
    "        \n",
    "        \n",
    "        # test\n",
    "        ts_test = (df\n",
    "                   .query('id > @cutoff')\n",
    "                   .rename(columns={names[1]: 'ds', names[2]: 'y'})\n",
    "                   .sort_values('ds')\n",
    "                   .assign(ds=lambda x: pd.to_datetime(x[\"ds\"]))\n",
    "                   .drop('y', axis=1)\n",
    "                   )[['ds']]\n",
    "        \n",
    "        print(ts_test.columns)\n",
    "\n",
    " \n",
    "\n",
    "        # init model\n",
    "        m = Prophet(yearly_seasonality=True,\n",
    "                    weekly_seasonality=True,\n",
    "                    daily_seasonality=True)\n",
    "        m.fit(ts_train)\n",
    "        \n",
    "        \n",
    "\n",
    "        # to date\n",
    "        \n",
    "        # at this step we predict the future and we get plenty of additional columns be cautious\n",
    "        ts_hat = (m.predict(ts_test)[[\"ds\", \"yhat\"]]\n",
    "                  .assign(ds=lambda x: pd.to_datetime(x[\"ds\"]))\n",
    "                  ).merge(ts_test, on=[\"ds\"], how=\"left\")  \n",
    "        \n",
    "\n",
    "        return pd.DataFrame(ts_hat, columns=schema.fieldNames())\n",
    "\n",
    "    return train_fitted_prophet(df, cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37f02f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyncoln/spark/python/pyspark/sql/pandas/group_ops.py:81: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    spark = (SparkSession\n",
    "             .builder\n",
    "             .appName(\"forecasting\")\n",
    "             .getOrCreate()\n",
    "             #.config('spark.sql.execution.arrow.enable', 'true')\n",
    "             )\n",
    "    \n",
    "    data = (spark\n",
    "                .read\n",
    "                .format(\"csv\")\n",
    "                .option('header', 'true')\n",
    "                .option('inferSchema','true')\n",
    "                .load('Downloads/AEP_hourly.csv')\n",
    "            )\n",
    "    \n",
    "    data.createOrReplaceTempView(\"data\")\n",
    "    data = spark.sql(f\"SELECT LEFT(Datetime,10) AS Datetime, {data.columns[1]}  FROM data\")\n",
    "    data = data.groupBy(\"Datetime\")\\\n",
    "               .mean(\"AEP_MW\")\\\n",
    "               .sort(col('DateTime'))\n",
    "    \n",
    "    data_length = data.count()\n",
    "    train_size = int(round(0.7 * data_length,0))\n",
    "    \n",
    "    \n",
    "    ##Add future days to predict\n",
    "    \n",
    "    last_day = data.tail(1)[0].__getitem__(\"Datetime\")  # Não sei se é viável\n",
    "    future_days = pd.date_range(start = last_day, periods = 29)\n",
    "    sequence_days = list(future_days.strftime(\"%Y-%m-%d\"))[1:-1]\n",
    "    future = spark.createDataFrame(sequence_days, StringType())\n",
    "    future.createOrReplaceTempView(\"future\")\n",
    "    future = spark.sql(\"SELECT value AS Datetime FROM future\")\n",
    "    future = future.withColumn(data.columns[1],lit(None))\n",
    "    \n",
    "\n",
    "    \n",
    "    df = (data.union(future)).sort(col('Datetime'))\n",
    "    df = dfZipWithIndex(df,colName=\"id\")\n",
    "    \n",
    "    \n",
    "    # 70% of the real dataset\n",
    "    #cutoff = '2014-06-08'\n",
    "    cutoff = train_size\n",
    "    # Apply forcasting\n",
    "    global_predictions = (df\n",
    "                          .groupBy()\n",
    "                          .apply(fit_pandas_udf)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d099d7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Index(['ds', 'y'], dtype='object')                                  (0 + 1) / 1]\n",
      "Index(['ds'], dtype='object')\n",
      "Initial log joint probability = -31.8657\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99       8759.76    0.00335605       1890.93           1           1      127   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     199       8784.39    0.00162212       143.623      0.8158      0.8158      248   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     299       8786.88   0.000171334       197.427           1           1      383   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     326       8787.95   9.80295e-05       338.505   4.674e-07       0.001      463  LS failed, Hessian reset \n",
      "     380       8788.65   1.91345e-05       93.1063   1.957e-07       0.001      578  LS failed, Hessian reset \n",
      "     399       8788.77   0.000540829        71.529           1           1      606   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     444       8788.86   4.42227e-05       166.534    4.17e-07       0.001      699  LS failed, Hessian reset \n",
      "     480        8788.9   1.97386e-05       84.6987   2.935e-07       0.001      791  LS failed, Hessian reset \n",
      "     499        8788.9   6.16923e-06        66.993      0.5301      0.5301      816   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     533       8789.35    9.2339e-05       322.293    3.94e-07       0.001      890  LS failed, Hessian reset \n",
      "     599       8790.01   0.000153384       165.975           1           1      976   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     642       8790.05    0.00010211       102.306   1.337e-06       0.001     1078  LS failed, Hessian reset \n",
      "     662       8790.05   2.62697e-07       56.9736      0.3712           1     1111   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|        ds|              yhat|\n",
      "+----------+------------------+\n",
      "|2014-06-09| 15679.71994478022|\n",
      "|2014-06-10|16019.616327382431|\n",
      "|2014-06-11| 16039.62830482524|\n",
      "|2014-06-12| 16085.79242202189|\n",
      "|2014-06-13|15832.072068242376|\n",
      "|2014-06-14|14656.448318788878|\n",
      "|2014-06-15|14250.972644239979|\n",
      "|2014-06-16| 15924.60862734608|\n",
      "|2014-06-17|16197.115709348192|\n",
      "|2014-06-18|16152.375409076561|\n",
      "|2014-06-19|16137.985126126703|\n",
      "|2014-06-20|15829.371756925782|\n",
      "|2014-06-21|14605.843465873702|\n",
      "|2014-06-22| 14160.60679389803|\n",
      "|2014-06-23|15803.577242323805|\n",
      "|2014-06-24|16055.237228472808|\n",
      "|2014-06-25|15999.942582784812|\n",
      "|2014-06-26|15985.503255891357|\n",
      "|2014-06-27|15687.290836482642|\n",
      "|2014-06-28|14484.291398089717|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "global_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823f63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(col('id')==train_size).show()14484.291398089717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7787a37d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
