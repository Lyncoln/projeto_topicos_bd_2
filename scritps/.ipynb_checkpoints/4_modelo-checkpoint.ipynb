{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1119d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, max, col, lit\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import Row\n",
    "\n",
    "\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "from fbprophet import Prophet\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b842505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfZipWithIndex (df, offset=1, colName=\"rowId\"):\n",
    "    '''\n",
    "        Enumerates dataframe rows is native order, like rdd.ZipWithIndex(), but on a dataframe \n",
    "        and preserves a schema\n",
    "\n",
    "        :param df: source dataframe\n",
    "        :param offset: adjustment to zipWithIndex()'s index\n",
    "        :param colName: name of the index column\n",
    "    '''\n",
    "\n",
    "    new_schema = StructType(\n",
    "                    [StructField(colName,LongType(),True)]        # new added field in front\n",
    "                    + df.schema.fields                            # previous schema\n",
    "                )\n",
    "\n",
    "    zipped_rdd = df.rdd.zipWithIndex()\n",
    "\n",
    "    new_rdd = zipped_rdd.map(lambda args: ([args[1] + offset] + list(args[0])))\n",
    "\n",
    "    return spark.createDataFrame(new_rdd, new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1649958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "        StructField(\"ds\", DateType(), True),\n",
    "        StructField(\"y\", DoubleType(), True),\n",
    "        StructField(\"yhat\", DoubleType(), True),\n",
    "        StructField(\"rmse\", DoubleType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf80db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def fit_pandas_udf(df):\n",
    "    \"\"\"\n",
    "    :param df: Dataframe (train + test data)\n",
    "    :return: predictions as defined in the output schema\n",
    "    \"\"\"\n",
    "\n",
    "    def train_fitted_prophet(df, cutoff):\n",
    "        \n",
    "        names = df.columns\n",
    "        \n",
    "        #train\n",
    "        ts_train = (df\n",
    "                    .query('id <= @cutoff')\n",
    "                    .rename(columns={names[1]: 'ds', names[2]: 'y'})\n",
    "                    .sort_values('ds')\n",
    "                    )[['ds','y']]\n",
    "        \n",
    "        print(ts_train.columns)\n",
    "        \n",
    "        \n",
    "        # test\n",
    "        ts_test = (df\n",
    "                   .query('id > @cutoff')\n",
    "                   .rename(columns={names[1]: 'ds', names[2]: 'y'})\n",
    "                   .sort_values('ds')\n",
    "                   .assign(ds=lambda x: pd.to_datetime(x[\"ds\"]))\n",
    "                   #.drop('y', axis=1)\n",
    "                   )[['ds','y']]\n",
    "        \n",
    "        print(ts_test.columns)\n",
    "        print(ts_test.ds.values[-28])\n",
    "        \n",
    "        floor = ts_train.y.min()*0.8\n",
    "        cap = ts_train.y.max()*3    \n",
    "        \n",
    "        print(floor,cap)\n",
    "        \n",
    "        ts_train['floor'], ts_train['cap'] = floor, cap\n",
    "        ts_test['floor'], ts_test['cap'] = floor, cap\n",
    "\n",
    "        # init model\n",
    "        m = Prophet(growth='logistic',\n",
    "                    yearly_seasonality=True,\n",
    "                    weekly_seasonality=True,\n",
    "                    daily_seasonality=True)\n",
    "        m.fit(ts_train)\n",
    "        \n",
    "        print(\"train\",ts_train.columns)\n",
    "        \n",
    "        ts_hat = (m.predict(ts_test)\n",
    "                  .assign(ds=lambda x: pd.to_datetime(x[\"ds\"]))\n",
    "                  ).merge(ts_test, on=[\"ds\"], how=\"left\")  \n",
    "        \n",
    "        \n",
    "        \n",
    "        #calculando rmse\n",
    "        \n",
    "        se = np.square(ts_hat.y[0:-28] - ts_hat.yhat[0:-28])\n",
    "        mse = np.mean(se)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        ts_hat['rmse'] = rmse\n",
    "        \n",
    "        ts_train['yhat'] = None\n",
    "        ts_train = ts_train.assign(ds=lambda x: pd.to_datetime(x[\"ds\"]))\n",
    "        \n",
    "        ts = ts_train[['ds','y','yhat']].append(ts_hat[['ds','y','yhat']])\n",
    "        \n",
    "        ts['rmse'] = rmse\n",
    "        \n",
    "        print(ts_hat.columns)\n",
    "        \n",
    "        return pd.DataFrame(ts, columns=schema.fieldNames())\n",
    "\n",
    "    return train_fitted_prophet(df, cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37f02f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyncoln/spark/python/pyspark/sql/pandas/group_ops.py:81: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    spark = (SparkSession\n",
    "             .builder\n",
    "             .appName(\"forecasting\")\n",
    "             .getOrCreate()\n",
    "             #.config('spark.sql.execution.arrow.enable', 'true')\n",
    "             )\n",
    "    \n",
    "   # data = (spark\n",
    "   #             .read\n",
    "   #             .format(\"csv\")\n",
    "   #             .option('header', 'true')\n",
    "   #             .option('inferSchema','true')\n",
    "   #             .load('data_simulation.csv')\n",
    "   #             #.load('Downloads/AEP_hourly.csv')\n",
    "   #             \n",
    "   #             \n",
    "   #         )\n",
    "    \n",
    "    data =  spark\\\n",
    "            .read\\\n",
    "            .option(\"header\",\"false\")\\\n",
    "            .option('inferSchema','true')\\\n",
    "            .csv(f\"{os.getcwd()}/dados_stream/part*.csv\")\\\n",
    "            .selectExpr(\"_c0 as Datetime\",\"_c1 as MW\")\\\n",
    "            .sort(col(\"Datetime\"))\n",
    "    \n",
    "    data.createOrReplaceTempView(\"data\")\n",
    "    data = spark.sql(f\"SELECT LEFT(Datetime,10) AS Datetime, {data.columns[1]}  FROM data\")\n",
    "    data = data.groupBy(\"Datetime\")\\\n",
    "               .mean(\"MW\")\\\n",
    "               .sort(col('DateTime'))\n",
    "    \n",
    "    \n",
    "    # 70% of the real dataset\n",
    "    data_length = data.count()\n",
    "    train_size = int(round(0.7 * data_length,0))\n",
    "    \n",
    "    \n",
    "    ##Add future days to predict\n",
    "    \n",
    "    #last_day = data.tail(1)[0].__getitem__(\"Datetime\")  # Não sei se é viável\n",
    "    last_day = data.tail(1)[0].asDict()['Datetime']\n",
    "    future_days = pd.date_range(start = last_day,\n",
    "                                periods = 28)\n",
    "    sequence_days = list(future_days.strftime(\"%Y-%m-%d\"))[1:]\n",
    "    future = spark.createDataFrame(sequence_days, \n",
    "                                   StringType())\n",
    "    future.createOrReplaceTempView(\"future\")\n",
    "    future = spark.sql(\"SELECT value AS Datetime FROM future\")\n",
    "    future = future.withColumn(data.columns[1],\n",
    "                               lit(None))\n",
    "    \n",
    "\n",
    "    \n",
    "    df = (data.union(future)).sort(col('Datetime'))\n",
    "    df = dfZipWithIndex(df,colName=\"id\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    cutoff = train_size\n",
    "    # Apply forcasting\n",
    "    global_predictions = (df\n",
    "                          .groupBy()\n",
    "                          .apply(fit_pandas_udf)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d099d7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_predictions.show(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a0ffec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_predictions\\\n",
    "    .write.mode('overwrite')\\\n",
    "    .option('hearder','true')\\\n",
    "    .csv('time_serie.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
