{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1119d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, max, col, lit\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import Row\n",
    "\n",
    "\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "from fbprophet import Prophet\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b842505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfZipWithIndex (df, offset=1, colName=\"rowId\"):\n",
    "    '''\n",
    "        Enumerates dataframe rows is native order, like rdd.ZipWithIndex(), but on a dataframe \n",
    "        and preserves a schema\n",
    "\n",
    "        :param df: source dataframe\n",
    "        :param offset: adjustment to zipWithIndex()'s index\n",
    "        :param colName: name of the index column\n",
    "    '''\n",
    "\n",
    "    new_schema = StructType(\n",
    "                    [StructField(colName,LongType(),True)]        # new added field in front\n",
    "                    + df.schema.fields                            # previous schema\n",
    "                )\n",
    "\n",
    "    zipped_rdd = df.rdd.zipWithIndex()\n",
    "\n",
    "    new_rdd = zipped_rdd.map(lambda args: ([args[1] + offset] + list(args[0])))\n",
    "\n",
    "    return spark.createDataFrame(new_rdd, new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1649958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "        StructField(\"ds\", DateType(), True),\n",
    "        StructField(\"y\", DoubleType(), True),\n",
    "        StructField(\"yhat\", DoubleType(), True),\n",
    "        StructField(\"rmse\", DoubleType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bf80db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def fit_pandas_udf(df):\n",
    "    \"\"\"\n",
    "    :param df: Dataframe (train + test data)\n",
    "    :return: predictions as defined in the output schema\n",
    "    \"\"\"\n",
    "\n",
    "    def train_fitted_prophet(df, cutoff):\n",
    "        \n",
    "        names = df.columns\n",
    "        \n",
    "        #train\n",
    "        ts_train = (df\n",
    "                    .query('id <= @cutoff')\n",
    "                    .rename(columns={names[1]: 'ds', names[2]: 'y'})\n",
    "                    .sort_values('ds')\n",
    "                    )[['ds','y']]\n",
    "        \n",
    "        print(ts_train.columns)\n",
    "        \n",
    "        \n",
    "        # test\n",
    "        ts_test = (df\n",
    "                   .query('id > @cutoff')\n",
    "                   .rename(columns={names[1]: 'ds', names[2]: 'y'})\n",
    "                   .sort_values('ds')\n",
    "                   .assign(ds=lambda x: pd.to_datetime(x[\"ds\"]))\n",
    "                   #.drop('y', axis=1)\n",
    "                   )[['ds','y']]\n",
    "        \n",
    "        print(ts_test.columns)\n",
    "        print(ts_test.ds.values[-28])\n",
    "        \n",
    "        floor = ts_train.y.min()*0.8\n",
    "        cap = ts_train.y.max()*3    \n",
    "        \n",
    "        print(floor,cap)\n",
    "        \n",
    "        ts_train['floor'], ts_train['cap'] = floor, cap\n",
    "        ts_test['floor'], ts_test['cap'] = floor, cap\n",
    "\n",
    "        # init model\n",
    "        m = Prophet(growth='logistic',\n",
    "                    yearly_seasonality=True,\n",
    "                    weekly_seasonality=True,\n",
    "                    daily_seasonality=True)\n",
    "        m.fit(ts_train)\n",
    "        \n",
    "        print(\"train\",ts_train.columns)\n",
    "        \n",
    "        ts_hat = (m.predict(ts_test)\n",
    "                  .assign(ds=lambda x: pd.to_datetime(x[\"ds\"]))\n",
    "                  ).merge(ts_test, on=[\"ds\"], how=\"left\")  \n",
    "        \n",
    "        \n",
    "        \n",
    "        #calculando rmse\n",
    "        \n",
    "        se = np.square(ts_hat.y[0:-28] - ts_hat.yhat[0:-28])\n",
    "        mse = np.mean(se)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        ts_hat['rmse'] = rmse\n",
    "        \n",
    "        ts_train['yhat'] = None\n",
    "        ts_train = ts_train.assign(ds=lambda x: pd.to_datetime(x[\"ds\"]))\n",
    "        \n",
    "        ts = ts_train[['ds','y','yhat']].append(ts_hat[['ds','y','yhat']])\n",
    "        \n",
    "        ts['rmse'] = rmse\n",
    "        \n",
    "        print(ts_hat.columns)\n",
    "        \n",
    "        return pd.DataFrame(ts, columns=schema.fieldNames())\n",
    "\n",
    "    return train_fitted_prophet(df, cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37f02f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyncoln/spark/python/pyspark/sql/pandas/group_ops.py:81: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    spark = (SparkSession\n",
    "             .builder\n",
    "             .appName(\"forecasting\")\n",
    "             .getOrCreate()\n",
    "             #.config('spark.sql.execution.arrow.enable', 'true')\n",
    "             )\n",
    "    \n",
    "   # data = (spark\n",
    "   #             .read\n",
    "   #             .format(\"csv\")\n",
    "   #             .option('header', 'true')\n",
    "   #             .option('inferSchema','true')\n",
    "   #             .load('data_simulation.csv')\n",
    "   #             #.load('Downloads/AEP_hourly.csv')\n",
    "   #             \n",
    "   #             \n",
    "   #         )\n",
    "    \n",
    "    data =  spark\\\n",
    "            .read\\\n",
    "            .option(\"header\",\"false\")\\\n",
    "            .option('inferSchema','true')\\\n",
    "            .csv(f\"{os.getcwd()}/dados_stream/part*.csv\")\\\n",
    "            .selectExpr(\"_c0 as Datetime\",\"_c1 as MW\")\\\n",
    "            .sort(col(\"Datetime\"))\n",
    "    \n",
    "    data.createOrReplaceTempView(\"data\")\n",
    "    data = spark.sql(f\"SELECT LEFT(Datetime,10) AS Datetime, {data.columns[1]}  FROM data\")\n",
    "    data = data.groupBy(\"Datetime\")\\\n",
    "               .mean(\"MW\")\\\n",
    "               .sort(col('DateTime'))\n",
    "    \n",
    "    \n",
    "    # 70% of the real dataset\n",
    "    data_length = data.count()\n",
    "    train_size = int(round(0.7 * data_length,0))\n",
    "    \n",
    "    \n",
    "    ##Add future days to predict\n",
    "    \n",
    "    #last_day = data.tail(1)[0].__getitem__(\"Datetime\")  # Não sei se é viável\n",
    "    last_day = data.tail(1)[0].asDict()['Datetime']\n",
    "    future_days = pd.date_range(start = last_day,\n",
    "                                periods = 28)\n",
    "    sequence_days = list(future_days.strftime(\"%Y-%m-%d\"))[1:]\n",
    "    future = spark.createDataFrame(sequence_days, \n",
    "                                   StringType())\n",
    "    future.createOrReplaceTempView(\"future\")\n",
    "    future = spark.sql(\"SELECT value AS Datetime FROM future\")\n",
    "    future = future.withColumn(data.columns[1],\n",
    "                               lit(None))\n",
    "    \n",
    "\n",
    "    \n",
    "    df = (data.union(future)).sort(col('Datetime'))\n",
    "    df = dfZipWithIndex(df,colName=\"id\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    cutoff = train_size\n",
    "    # Apply forcasting\n",
    "    global_predictions = (df\n",
    "                          .groupBy()\n",
    "                          .apply(fit_pandas_udf)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d099d7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_predictions.show(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7a0ffec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Index(['ds', 'y'], dtype='object')                                  (0 + 1) / 1]\n",
      "Index(['ds', 'y'], dtype='object')\n",
      "2007-06-27T00:00:00.000000000\n",
      "6676.647619047619 52332.875\n",
      "Initial log joint probability = -12.6529\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99       1497.92    0.00251952        659.13      0.1419           1      148   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     199       1501.56    0.00262777       535.167      0.7913      0.7913      282   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     233       1502.69   1.23294e-05        85.249   1.116e-07       0.001      390  LS failed, Hessian reset \n",
      "     286       1502.94   1.19403e-05       81.4255   1.092e-07       0.001      515  LS failed, Hessian reset \n",
      "     299       1502.96   8.58874e-05       111.903           1           1      535   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     323        1503.1   3.73448e-05        105.81   6.848e-08       0.001      602  LS failed, Hessian reset \n",
      "     399       1504.49   0.000361902       326.126      0.6153      0.6153      713   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     467       1504.79    9.7822e-06        64.331   6.167e-08       0.001      847  LS failed, Hessian reset \n",
      "     489       1504.79    1.8773e-05       51.5482   2.606e-07       0.001      911  LS failed, Hessian reset \n",
      "     499       1504.79   1.59044e-06       65.5785       0.812       0.812      923   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     511       1504.79   1.03708e-07       53.6934       1.137      0.2877      943   \n",
      "Optimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\n",
      "train Index(['ds', 'y', 'floor', 'cap'], dtype='object')\n",
      "Index(['ds', 'trend', 'cap_x', 'floor_x', 'yhat_lower', 'yhat_upper',\n",
      "       'trend_lower', 'trend_upper', 'additive_terms', 'additive_terms_lower',\n",
      "       'additive_terms_upper', 'daily', 'daily_lower', 'daily_upper', 'weekly',\n",
      "       'weekly_lower', 'weekly_upper', 'yearly', 'yearly_lower',\n",
      "       'yearly_upper', 'multiplicative_terms', 'multiplicative_terms_lower',\n",
      "       'multiplicative_terms_upper', 'yhat', 'y', 'floor_y', 'cap_y', 'rmse'],\n",
      "      dtype='object')\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "global_predictions\\\n",
    "    .write.mode('overwrite')\\\n",
    "    .option('header','true')\\\n",
    "    .csv('time_serie')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
