{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, sum, max, col, concat, lit, monotonically_increasing_id\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import Row\n",
    "\n",
    "\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "from fbprophet import Prophet\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b842505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfZipWithIndex (df, offset=1, colName=\"rowId\"):\n",
    "    '''\n",
    "        Enumerates dataframe rows is native order, like rdd.ZipWithIndex(), but on a dataframe \n",
    "        and preserves a schema\n",
    "\n",
    "        :param df: source dataframe\n",
    "        :param offset: adjustment to zipWithIndex()'s index\n",
    "        :param colName: name of the index column\n",
    "    '''\n",
    "\n",
    "    new_schema = StructType(\n",
    "                    [StructField(colName,LongType(),True)]        # new added field in front\n",
    "                    + df.schema.fields                            # previous schema\n",
    "                )\n",
    "\n",
    "    zipped_rdd = df.rdd.zipWithIndex()\n",
    "\n",
    "    new_rdd = zipped_rdd.map(lambda args: ([args[1] + offset] + list(args[0])))\n",
    "\n",
    "    return spark.createDataFrame(new_rdd, new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1649958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "        StructField(\"ds\", DateType(), True),\n",
    "        StructField(\"yhat\", DoubleType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf80db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def fit_pandas_udf(df):\n",
    "    \"\"\"\n",
    "    :param df: Dataframe (train + test data)\n",
    "    :return: predictions as defined in the output schema\n",
    "    \"\"\"\n",
    "\n",
    "    def train_fitted_prophet(df, cutoff):\n",
    "        \n",
    "        names = df.columns\n",
    "        \n",
    "        #train\n",
    "        ts_train = (df\n",
    "                    .query('id <= @cutoff')\n",
    "                    .rename(columns={names[1]: 'ds', names[2]: 'y'})\n",
    "                    .sort_values('ds')\n",
    "                    )[['ds','y']]\n",
    "        \n",
    "        print(ts_train.columns)\n",
    "        \n",
    "        \n",
    "        # test\n",
    "        ts_test = (df\n",
    "                   .query('id > @cutoff')\n",
    "                   .rename(columns={names[1]: 'ds', names[2]: 'y'})\n",
    "                   .sort_values('ds')\n",
    "                   .assign(ds=lambda x: pd.to_datetime(x[\"ds\"]))\n",
    "                   .drop('y', axis=1)\n",
    "                   )[['ds']]\n",
    "        \n",
    "        print(ts_test.columns)\n",
    "\n",
    " \n",
    "\n",
    "        # init model\n",
    "        m = Prophet(yearly_seasonality=True,\n",
    "                    weekly_seasonality=True,\n",
    "                    daily_seasonality=True)\n",
    "        m.fit(ts_train)\n",
    "        \n",
    "        \n",
    "\n",
    "        # to date\n",
    "        \n",
    "        # at this step we predict the future and we get plenty of additional columns be cautious\n",
    "        ts_hat = (m.predict(ts_test)[[\"ds\", \"yhat\"]]\n",
    "                  .assign(ds=lambda x: pd.to_datetime(x[\"ds\"]))\n",
    "                  ).merge(ts_test, on=[\"ds\"], how=\"left\")  \n",
    "        \n",
    "\n",
    "        return pd.DataFrame(ts_hat, columns=schema.fieldNames())\n",
    "\n",
    "    return train_fitted_prophet(df, cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f02f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    spark = (SparkSession\n",
    "             .builder\n",
    "             .appName(\"forecasting\")\n",
    "             .getOrCreate()\n",
    "             #.config('spark.sql.execution.arrow.enable', 'true')\n",
    "             )\n",
    "    \n",
    "    data = (spark\n",
    "                .read\n",
    "                .format(\"csv\")\n",
    "                .option('header', 'true')\n",
    "                .option('inferSchema','true')\n",
    "                .load('Downloads/AEP_hourly.csv')\n",
    "                #.load('data_simulation.csv')\n",
    "                \n",
    "            )\n",
    "    \n",
    "    data.createOrReplaceTempView(\"data\")\n",
    "    data = spark.sql(f\"SELECT LEFT(Datetime,10) AS Datetime, {data.columns[1]}  FROM data\")\n",
    "    data = data.groupBy(\"Datetime\")\\\n",
    "               .mean(\"AEP_MW\")\\\n",
    "               .sort(col('DateTime'))\n",
    "    \n",
    "    \n",
    "    # 70% of the real dataset\n",
    "    data_length = data.count()\n",
    "    train_size = int(round(0.7 * data_length,0))\n",
    "    \n",
    "    \n",
    "    ##Add future days to predict\n",
    "    \n",
    "    #last_day = data.tail(1)[0].__getitem__(\"Datetime\")  # Não sei se é viável\n",
    "    last_day = data.tail(1)[0].asDict()['Datetime']\n",
    "    future_days = pd.date_range(start = last_day,\n",
    "                                periods = 28)\n",
    "    sequence_days = list(future_days.strftime(\"%Y-%m-%d\"))[2:-1]\n",
    "    future = spark.createDataFrame(sequence_days, \n",
    "                                   StringType())\n",
    "    future.createOrReplaceTempView(\"future\")\n",
    "    future = spark.sql(\"SELECT value AS Datetime FROM future\")\n",
    "    future = future.withColumn(data.columns[1],\n",
    "                               lit(None))\n",
    "    \n",
    "\n",
    "    \n",
    "    df = (data.union(future)).sort(col('Datetime'))\n",
    "    df = dfZipWithIndex(df,colName=\"id\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    cutoff = train_size\n",
    "    # Apply forcasting\n",
    "    global_predictions = (df\n",
    "                          .groupBy()\n",
    "                          .apply(fit_pandas_udf)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d099d7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_predictions.show(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85278654",
   "metadata": {},
   "outputs": [],
   "source": [
    "    data = (spark\n",
    "                .read\n",
    "                .format(\"csv\")\n",
    "                .option('header', 'true')\n",
    "                .option('inferSchema','true')\n",
    "                .load('data_simulation.csv')\n",
    "                #.load('Downloads/AEP_hourly.csv')\n",
    "            )\n",
    "    data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617b0335",
   "metadata": {},
   "outputs": [],
   "source": [
    "allfiles =  spark\\\n",
    "            .read\\\n",
    "            .option(\"header\",\"false\")\\\n",
    "            .option('inferSchema','true')\\\n",
    "            .csv(f\"{os.getcwd()}/abc/part*.csv\")\\\n",
    "            .selectExpr(\"_c0 as Datetime\",\"_c1 as MW\")\\\n",
    "            .sort(col(\"Datetime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601251cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "allfiles.groupBy(\"Datetime\").mean(\"MW\").show(100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
